---
title: "Unit 4 Assignment Voting"
author: "Vinyas"
date: "16 May 2016"
output: html_document
---

#####Loading data
```{r load}
gerber=read.csv("Data Files/gerber.csv")
summary(gerber)
```

#####What proportion of people in this dataset voted in this election?
```{r 1}
as.numeric(table(gerber$voting)[2]/nrow(gerber))


```
#####Which of the four "treatment groups" had the largest percentage of people who actually voted (voting = 1)?
```{r 1.2}
tapply(gerber$voting, gerber$civicduty, mean)

tapply(gerber$voting, gerber$hawthorne, mean)

tapply(gerber$voting, gerber$self, mean)

tapply(gerber$voting, gerber$neighbors, mean)
```

####Logistic regression model
```{r 1.3}
gerber_model=glm(voting~hawthorne+civicduty+neighbors+self,data=gerber,family = binomial)
summary(gerber_model)
```

#####Using a threshold of 0.3 and 0.5, what is the accuracy of the logistic regression model?
```{r 1.4}
predict_voting=predict(gerber_model,type="response")

table(gerber$voting,predict_voting>0.3)

sum(diag(table(gerber$voting,predict_voting>0.3)))/nrow(gerber) #Accuracy
```
#####Compare your previous two answers to the percentage of people who did not vote (the baseline accuracy) and compute the AUC of the model. What is happening here?

```{r 1.6}
library(ROCR)
ROCRpred=prediction(predict_voting,gerber$voting)
as.numeric(performance(ROCRpred,"auc")@y.values)

#Even though all of our variables are significant, our model does not improve over the baseline model of just predicting that someone will not vote, and the AUC is low. So while the treatment groups do make a difference, this is a weak predictive model.
```

######Build a CART tree for voting using all data and the same four treatment variables we used before. Don't set the option method="class" - we are actually going to create a regression tree here. We are interested in building a tree to explore the fraction of people who vote, or the probability of voting.

```{r 2.1}
library(rpart)
library(rpart.plot)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(CARTmodel)

#you should just see one leaf! There are no splits in the tree, because none of the variables make a big enough effect to be split on.
```

```{r 2.2}

#force the complete tree to be built

CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)

```
```{r 2.4}
#NEw model with the variable sex added
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(CARTmodel3)
```

#####Make new models, one with only control as independent var and second with control and sex
```{r 3.1}

CARTmodel4 = rpart(voting ~ control, data=gerber, cp=0.0)
CARTmodel5 = rpart(voting ~ control+sex, data=gerber, cp=0.0)

prp(CARTmodel4,digits=6)
abs(0.296638-0.34)
```

#####create a logistic regression model using "sex" and "control". Interpret the coefficient for "sex"
```{r 3.3}
gerber_model2=glm(voting~control+sex,data=gerber,family = binomial)
summary(gerber_model2)


#you can see that the coefficient for the "sex" variable is -0.055791. This means that women are less likely to vote, since women have a larger value in the sex variable, and a negative coefficient means that larger values are predictive of 0.

Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(gerber_model2, newdata=Possibilities, type="response")
```

#####We're going to add a new term to our logistic regression now, that is the combination of the "sex" and "control" variables - so if this new variable is 1, that means the person is a woman AND in the control group
```{r 3.5}

gerber_model3=glm(voting~sex+control+sex:control,data = gerber,family='binomial')
summary(gerber_model3)

#This coefficient is negative, so that means that a value of 1 in this variable decreases the chance of voting. This variable will have variable 1 if the person is a woman and in the control group.
```

```{r 3.6}
predict(gerber_model3, newdata=Possibilities, type="response")

#The logistic regression model now predicts 0.2904558 for the (Woman, Control) case, so there is now a very small difference (practically zero) between CART and logistic regression.
```

######We should not use all possible interaction terms in a logistic regression model due to overfitting. Even in this simple problem, we have four treatment groups and two values for sex. If we have an interaction term for every treatment variable with sex, we will double the number of variables. In smaller data sets, this could quickly lead to overfitting.